# -*- coding: utf-8 -*-
"""Tarea_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ua2kyemSb484jmDwhIqWMypiVMpiFU0e
"""

from google.colab import drive
drive.mount('/content/drive')

# Copiar el archivo a tu entorno de trabajo
!cp /content/drive/MyDrive/Steam_2024_bestRevenue_1500.csv /content/

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Leer el archivo CSV
df = pd.read_csv('/content/Steam_2024_bestRevenue_1500.csv')

# Mostrar las primeras filas del DataFrame
df.head()

# Ver el número de observaciones (filas) y columnas
df.shape

# Obtener estadísticas descriptivas para las columnas numéricas
df.describe()

# Ver los tipos de datos de cada columna
df.info()

# Contar los valores únicos en columnas categóricas
print(df['publisherClass'].value_counts())
print(df['publishers'].value_counts())

# Seleccionar solo las columnas numéricas
numerical_df = df.select_dtypes(include=['float64', 'int64'])

# Calcular la matriz de correlación
correlation_matrix = numerical_df.corr()

# Mostrar la matriz de correlación
correlation_matrix

# Mostrar la matriz de correlación
print(correlation_matrix)

# Eliminar columnas categóricas no deseadas excepto 'publisherClass'
df = df.drop(['name', 'releaseDate', 'publishers', 'developers', 'steamId'], axis=1)

# Aplicar transformación logarítmica a características con alta variabilidad
df['log_copiesSold'] = np.log1p(df['copiesSold'])

# Realizar One-Hot Encoding a la columna 'publisherClass'
df_encoded = pd.get_dummies(df, columns=['publisherClass'], drop_first=True)

# Definir las características (X) y la variable objetivo continua (y)
X = df_encoded.drop('revenue', axis=1)  # Todas las columnas menos 'revenue'
y = df_encoded['revenue']

# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X,y)

#Se importa Mlflow para registrar los experimentos
!pip install mlflow
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

mlflow.end_run()

# Configura la URI de MLflow para apuntar a tu servidor remoto
mlflow.set_tracking_uri("http://44.212.55.255:5000")

import pandas as pd
import numpy as np
import mlflow
import mlflow.sklearn
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Configurar el experimento en MLflow
experiment_name = "sklearn-diab"
experiment = mlflow.get_experiment_by_name(experiment_name)
if experiment is None:
    experiment_id = mlflow.create_experiment(experiment_name)
else:
    experiment_id = experiment.experiment_id

# Escalar las características numéricas
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Configurar la búsqueda en cuadrícula y encontrar los mejores parámetros
param_grid_rf_regressor = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 15, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 3]
}
rf_regressor = RandomForestRegressor(random_state=42)
grid_search_rf_regressor = GridSearchCV(estimator=rf_regressor, param_grid=param_grid_rf_regressor, cv=5, scoring='r2', n_jobs=-1)
grid_search_rf_regressor.fit(X_train_scaled, y_train)
best_rf_regressor = grid_search_rf_regressor.best_estimator_

# Registrar el experimento en MLflow
with mlflow.start_run(experiment_id=experiment_id):
    # Registrar los parámetros óptimos encontrados
    mlflow.log_param("n_estimators", best_rf_regressor.n_estimators)
    mlflow.log_param("max_depth", best_rf_regressor.max_depth)
    mlflow.log_param("min_samples_split", best_rf_regressor.min_samples_split)
    mlflow.log_param("min_samples_leaf", best_rf_regressor.min_samples_leaf)

    # Entrenar y hacer predicciones con el mejor modelo
    best_rf_regressor.fit(X_train_scaled, y_train)
    y_pred = best_rf_regressor.predict(X_test_scaled)

    # Calcular métricas y registrarlas en MLflow
    mae = mean_absolute_error(y_test, y_pred)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    r2 = r2_score(y_test, y_pred)

    print("MAE registrado:", mae)
    print("RMSE registrado:", rmse)
    print("R^2 registrado:", r2)

    mlflow.log_metric("mae", mae)
    mlflow.log_metric("rmse", rmse)
    mlflow.log_metric("r2", r2)

    # Registrar el modelo en MLflow con un ejemplo de entrada
    input_example = pd.DataFrame([X_test.iloc[0]])
    mlflow.sklearn.log_model(best_rf_regressor, "random-forest-model", input_example=input_example)

print("Experimento registrado con éxito en MLflow.")

# Configurar el experimento en MLflow
experiment_name = "sklearn-diab"
experiment = mlflow.get_experiment_by_name(experiment_name)
if experiment is None:
    experiment_id = mlflow.create_experiment(experiment_name)
else:
    experiment_id = experiment.experiment_id

# Ejecutar el experimento
with mlflow.start_run(experiment_id=experiment_id):
    # Definir los parámetros del modelo
    n_estimators = 500
    max_depth = 20
    min_samples_split = 3
    min_samples_leaf = 2

    # Registrar los parámetros en MLflow
    mlflow.log_param("n_estimators", n_estimators)
    mlflow.log_param("max_depth", max_depth)
    mlflow.log_param("min_samples_split", min_samples_split)
    mlflow.log_param("min_samples_leaf", min_samples_leaf)

    # Crear y entrenar el modelo
    model = RandomForestRegressor(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        random_state=42
    )
    model.fit(X_train, y_train)

    # Realizar predicciones
    y_pred = model.predict(X_test)

    # Calcular métricas y registrarlas en MLflow
    mae = mean_absolute_error(y_test, y_pred)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    r2 = r2_score(y_test, y_pred)

    print("MAE registrado:", mae)
    print("RMSE registrado:", rmse)
    print("R^2 registrado:", r2)

    mlflow.log_metric("mae", mae)
    mlflow.log_metric("rmse", rmse)
    mlflow.log_metric("r2", r2)

    # Registrar el modelo en MLflow
    mlflow.sklearn.log_model(model, "random-forest-model")

print("Experimento registrado con éxito en MLflow.")